<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="We scale diffusion models for language conditioned planning.">
  <meta name="keywords" content="planning,diffusion,language,RL,robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Large Legislative Models</title>



  <!-- custom fonts -->
  <link rel="stylesheet" type="text/css"
    href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
  <link href="https://fonts.cdnfonts.com/css/proxima-nova-2" rel="stylesheet">
  <!-- end custom fonts -->

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" type="ico" href="./static/images/humun.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>

<body>



  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Large Legislative Models: Towards Efficient AI Policymaking in Economic Simulations </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://uk.linkedin.com/in/henrygasztowtt">Henry Gasztowtt* </a><sup>1,2</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/benjamin-smith-46b184254/">Benjamin Smith*</a><sup>1, 3</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/vincent-zhu-6819a4224/">Vincent Zhu</a><sup>1, 3</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=p1tu16UAAAAJ&hl=en"> Qinxun Bai</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="https://eddie.win">Eddie
                  Zhang</a><sup>1, 5</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Humanity Unleashed</span>
              <span class="author-block"><sup>2</sup>Oxford University</span>
              <span class="author-block"><sup>3</sup>University of California Santa Barbara</span>
              <span class="author-block"><sup>4</sup>Horizon Robotics</span>
              <span class="author-block"><sup>5</sup>OpenAI</span>

            </div>

            <div class="is-size-5 publication-authors">
              Submitted to ICLR 2025
            </div>
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://arxiv.org/pdf/2410.08345"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a target="_blank" href="https://arxiv.org/abs/2410.08345"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a target="_blank" href="https://github.com/hegasz/large-legislative-models"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                
                
                <!-- 
                <span class="link-block">
                  <a target="_blank" href="https://thefounding.ai"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <img src="./static/images/logo.avif" alt="logo" style="height: 20px; width: auto;">
                    </span>
                    <span>The Founding</span>
                  </a>
                </span> -->

              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          Large Legislative Models proposes the usage of LLMs as a novel approach to AI-driven policymaking.
        </h2>
        <br />
        *Equal contribution.
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container has-text-centered">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width" style="display: flex; justify-content: center;">
            <img style="width: 200px; height: 150px;" src="./static/images/social_dilemma.png" alt="sedv1" class="initimages">
            <img style="width: 200px; height: 150px;" src="./static/images/cleanup.png" alt="sedv2" class="initimages">
            <img style="width: 200px; height: 150px;" src="./static/images/CER.png" alt="sedv2" class="initimages">
          </div>
        </div>
      </div>
    </div>
  </section>

  <div class="rounded-box"
    style="border-radius: 16px; padding: 10px; text-align: center; margin: 20px; width: 60%;; margin-left: auto; margin-right: auto;">
    <div class="content-toggle-buttons">
      <a id="btnTLDR" class="external-link button is-normal is-rounded active" onclick="showTLDR()">
        <span>Brief Summary</span>
      </a>
      <a id="btnBeginner" class="external-link button is-normal is-rounded" onclick="showBeginner()">
        <span>Beginner-Friendly Explanation</span>
      </a>
      <a id="btnDetailed" class="external-link button is-normal is-rounded" onclick="showDetailed()">
        <span>Detailed Explanation</span>
      </a>
    </div>
  </div>


  <div id="complex" style="display: none;">
    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                The improvement of economic policymaking presents an opportunity for broad societal benefit, a notion that has inspired research towards AI-driven policymaking tools.
               AI policymaking holds the potential to surpass human performance through the ability to process data quickly at scale. However, existing RL-based methods exhibit 
               sample inefficiency, and are further limited by an inability to flexibly incorporate nuanced information into their decision-making processes. Thus, we propose a novel 
               method in which we instead utilize pre-trained Large Language Models (LLMs), as sample-efficient policymakers in socially complex multi-agent reinforcement learning (MARL) 
               scenarios. We demonstrate significant efficiency gains, outperforming existing methods across three environments. 
               Our code is available at<a href="https://github.com/hegasz/large-legislative-models"> this link</a>.
              </p>
            </div>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Introduction. -->
      <div class="container is-max-desktop content">
        <div style="height: 50px;"></div>
        <h2 class="title is-3">
          <div class="toggle-button" onclick="toggleSection('intro')"
            style="cursor: pointer; display: flex; justify-content: space-between; align-items: center; width: 100%;">
            <span>Introduction</span>
            <span class="toggle-icon">+</span>
          </div>
        </h2>
        <div id="intro" class="collapse-content">
                  
        Economic policy-making is a field rife with uncertainty, high stakes, and complexity. Human policy-makers are often faced with overwhelming amounts of data 
        and the influence of vested interests , complicating effective and equitable decision-making. AI-driven tooling, with the ability to avoid self-centered bias
        and parse large amounts of data quickly, could offer significant assistance. 
        <br><br>
        Existing research  towards AI-based policymaking has primarily focused on reinforcement learning (RL) based methods using neural networks as economic policy
         generators* 
        A leading method in this field, termed AI Economist , attempts to maximize the social welfare of agents within a MARL environment called Gather-Trade-Build (GTB). 
        In GTB, agents can move around, collect wood and stone, and trade resources in order to gain reward. AI Economist employs a neural network that observes agent 
        endowments and market behavior, and outputs tax rates; for a fixed set of weights, this network is an economic policy generator – a function mapping observed 
        economic data to economic policy, such as tax rates. Policymaking unfolds as a bilevel optimization problem, where the outer loop trains the network's weights, 
        and each set of weights induces an economic policy generator parameterizing the environment that agents train against in the inner loop.
        <br><br>
        Though environment observations like agent endowments may contain useful information for the outer loop generator optimization, they are not necessarily variables that 
        economic policies should be a function of. Notably, we found that removing the generator network observations from both the AI Economist and 's MetaGrad, another leading method, 
        <i>had no effect on their performance</i>. This implies that optimal tax rates can be static in their environments, and agent endowments or market behavior are not needed as 
        input. However, such observations should inform the outer policymaking process. For AI Economist and MetaGrad, this information is limited to being used through the generator 
        network optimization step, where observations from an episode affect the subsequent update to generator network weights only through their gradient in the loss. Our ablations 
        demonstrate this is an overly complicated way to approach AI policymaking -- a problem that exacerbates the sample inefficiency that RL methods already suffer from. In our test 
        environments, both methods suffered from a starkly visible sample inefficiency problem; AI Economist was outperformed or matched by a much simpler &epsilon;-greedy bandit 
        algorithm on two of our three environments, and MetaGrad by all three bandit baselines we use across all test environments.
        <br><br>
        Motivated by these limitations, we propose a simpler alternative approach to AI-based policymaking by leveraging pre-trained Large Language Models (LLMs) as policymakers. 
        Instead of learning economic policy generators, we directly learn economic policy by applying the In-Context Learning (ICL)  capabilities of LLMs. In addition, as our method 
        uses a sequence modeling approach to predict policies as prompt completions, it is highly flexible in its inputs. For example, an economic report from human experts detailing 
        suggested tax rates could improve sample efficiency in finding an optimal tax rate policy. For an LLM, the entire report in natural language can be added to a prompt to inform 
        the policy subsequently chosen, while it is unclear how this would be done for existing methods without further augmentation. Furthermore, LLMs can also be given a contextualization 
        of the problem setting they are applied to, potentially allowing them to draw upon their extensive pre-training data distribution to improve sample efficiency and solve more complex, 
        realistic environments. Overall, our method is significantly more sample efficient than prior approaches. We outperform five baselines significantly in terms of sample efficiency across 
        three multi-agent test environments, with little compromise to final asymptotic performance.
        <br><br>
        In the following sections, we will discuss our method and experimental results. To read more about our ablations and formalizations of existing methods, we encourage you to refer to the full paper.
        <br><br><br>
        *Note that the word "policy" is overloaded. We use "economic policy" to refer to decisions made by a social planner, such as tax rates,
        synonymous with "fiscal policy". In contrast, we use "policy" to refer to the economic agents' strategy, synonymous with "reinforcement learning policy".
        </div>
        <!--/ Introduction. -->

        <!-- Example -->
        <div class="container is-max-desktop content">
          <h2 class="title is-3">
            <div class="toggle-button" onclick="toggleSection('method')"
              style="cursor: pointer; display: flex; justify-content: space-between; align-items: center; width: 100%;">
              <span>Our Method</span>
              <span class="toggle-icon">+</span>
            </div>
          </h2>
          <div id="method" class="collapse-content">
            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width">
                <img style="width: 1000px" src="./static/images/diagram.png" alt="diagram">
              </div>
            </div>
            <div>
              <strong>Figure 1: LLM Principal.</strong> At the beginning of each episode, 
              <strong>1.)</strong> A prompt is built from three components: <strong>Context</strong>, an overview of the problem setting; 
              <strong>Demonstrations</strong>, a documentation of previous principal actions and associated outcomes; and <strong>Query</strong>, 
              a request for the next action \( \phi \). 
              <strong>2.)</strong> An LLM takes in this prompt and produces \( \phi \). 
              <strong>3.)</strong> \( \phi \) induces a POMG \( \mathcal{M}^\phi \). Agents train until reaching best-response policy \( \pi^* \) under \( \phi \), 
              and we then evaluate \( (\phi, \pi^*) \) within \( \mathcal{M}^\phi \), yielding principal observation trajectory 
              \( \tau_P \in \Omega_P^* \) and principal payoff \( u_0(\phi, \pi^*) \). 
              <strong>4.)</strong> After evaluation, we process \( \tau_P \), extracting historical data beyond \( u_0(\phi, \pi^*) \). 
              <strong>5.)</strong> Payoff and historical data are appended to the prompt history.
            </div>
            
            
            <br />
            <div>
              <div class="container is-max-desktop content">
                <div>
                  We propose a simpler method using Large Language Models (LLMs) as economic policymakers (principals) to directly output economic policies (e.g., tax rates) used to parameterize MARL environments formalized as POMGs. 
                  This approach enables us to flexibly take in a wide variety of inputs beyond the current POMG state, focusing on contextualization and historical observations. 
                  We propose using the In-Context Learning (ICL) capabilities of the LLM to learn the optimal economic policy without updating any weights by leveraging 
                  contextualization and historical observation. 
                  <br /><br />
                  We define contextualizations as natural language descriptions of the problem setting to which our method is applied. Historical data is defined 
                  by extending induced POMGs \( \mathcal{M}^\phi \) in Stackelberg-Markov games to 
                  \( (S, A^{\phi}, T^{\phi}, r^{\phi}, \Omega^{\phi}_F, \Omega_P, O^{\phi}_F, O_P, \gamma^{\phi}, \mu_0^{\phi}) \), 
                  augmented with a non-parameterizable principal observation space \( \Omega_P \) and stochastic observation function 
                  \( O_P \), distinct from the follower observation space and observation function 
                  \( \Omega_F \) and \( O_F \) respectively.
                  <br /><br />
                  At the end of an episode of \( \mathcal{M}^\phi \), the principal POMG observation trajectory 
                  \( \tau_P \in \Omega_P^* := \bigcup_{i \geq 0} \Omega_P^i \) 
                  is summarized into "historical observations" to inform the next choice of principal action \( \phi' \). 
                  This contrasts with other methods that use historical observations to inform choices of \( \phi \) through network optimization steps. 
                  For more detail on the extended POMG, please refer to Appendix D in our paper. 
                  <br /><br />
                  Our LLM principal method, as illustrated in Figure 1, queries an LLM for an initial action choice conditioned on a contextualization of
                   the problem setting and iteratively adds to this contextualization, appending successive action choices and their corresponding payoffs and historical 
                  bservations. Additionally, to further increase the difficulty of our environments for GPT-4o mini, our strongest LLM, we occasionally include deliberate 
                  irrelevant information in contextualizations and historical observations received to test its ability to parse these effectively; the exact prompts we 
                  use are provided in the paper. Overall, the prompt structure for our method is as follows: 
                </div>
                <div style="text-align: center;">
                  <div style="color: red; font-weight: bold;">&lt;CONTEXT&gt;</div>
                  <div style="color: blue; font-weight: bold;">
                    &lt;ACTION<sub>1</sub>&gt;&lt;PAYOFF<sub>1</sub>&gt;&lt;HISTORICAL-OBSERVATION<sub>1</sub>&gt;
                  </div>
                  <div style="color: blue; font-weight: bold;">
                    ...
                  </div>
                  <div style="color: blue; font-weight: bold;">
                    &lt;ACTION<sub>n-1</sub>&gt;&lt;PAYOFF<sub>n-1</sub>&gt;&lt;HISTORICAL-OBSERVATION<sub>n-1</sub>&gt;
                  </div>
                  <div style="color: red; font-weight: bold;">&lt;QUERY&gt;</div>
                </div>
                Note the repition of Action, Payoff, and Historical-Observation, in a similar format to CoT prompting, but with historical observations rather than reasoning.
              </div>
            </div>
          </div>
          <!-- /Example -->
          <!-- Where Do We Go From Here?-->
          <div class="container is-max-desktop content">
            <h2 class="title is-3">
              <div class="toggle-button" onclick="toggleSection('experiments')"
                style="cursor: pointer; display: flex; justify-content: space-between; align-items: center; width: 100%;">
                <span>Experiments and Results</span>
                <span class="toggle-icon">+</span>
              </div>
            </h2>
            <div id="experiments" class="collapse-content">
                To demonstrate the effectiveness of our approach, we conduct experiments across three environments. Each environment was chosen to showcase social 
                dilemmas where agents without external influence achieve low social welfare. We rigorously validate the following baselines: AI Economist, MetaGrad, 
                and three prevalent bandit algorithms: UCB, Thompson sampling,  and &epsilon;-greedy. Our LLM method is tested across two models: GPT-4o mini and Gemini-1.5 flash.
                <br><br>
                <div style="overflow: auto;">
                  <img src="./static/images/social_dilemma.png"  alt="Harvest" style="width: 300px; float:right; margin-left: 20px;" />
                  <strong>Harvest</strong>: In this environment, apples grow in patches. Agents are given a reward when they pick an apple. Apples 
                  can regrow, but only if there are still other apples in the patch. This is the source of the social dilemma in this environment; 
                  if two agents are looking at the last apple in a patch, they each have an incentive to pick it and get their reward before the other agent.
                  This dilemma results in all of the apples being quickly harvested, and the agents are left with no apples to pick. In this environment, the goal of
                  the principal is to maximize the total number of apples harvested in the simulation by creating three tax rates. These tax rates apply to 
                  the reward signal of an agent that picks an apple based on how many apples they have recently harvested, and splits the taxed reward between all agents.
                </div>
                <br /><br />
                <div style="overflow: auto;">
                  <img src="./static/images/cleanup.png"  alt="Clean UP" style="width: 300px; float:right; margin-left: 20px;" />
                  <strong>Clean Up</strong>: Similarly to Harvest, this environment contains apples that give reward to agents that pick them. 
                  Here, apple regrowth is not linked to surrounding apples, but rather the level of pollution in the river, which builds throughout the simulation.
                  Too much pollution in the river means that apples won't grow back at all. Agents can clean up the river, but they are punished for doing so.
                  Without a principal, the agents harvest all available apples, but avoid cleaning the river. Again, the goal of the principals in this environment is to
                  maximize the total number of apples harvested in the simulation. In Clean Up, principals output three incentives that are added to the reward signal of
                  agents that harvest an apple, clean the river, or take some other action. A good set of incentives must carefully balance these incentives to ensure
                  that agents clean enough to ensure apple regrowth without ignoring harvesting <br/> altogether. 
                </div>
                <br /><br />
                <div style="overflow: auto;">
                  <img src="./static/images/CER.png"  alt="Clean UP" style="width: 300px; float:right; margin-left: 20px;" />
                  <strong>Contextual Escape Room</strong> (CER): In this environment, five agents are placed in a starting state,
                  and can move to four other states — three levers, and a door. In every simulation, one lever is randomly 'activated'. If two agents pull the activated lever, the door
                  opens, and any agents at the door receive a large reward. However, the agents at the lever are given a small punishment. As a result, agents without external influence
                  always go to the door, leaving none to pull the lever. In this environment, principals create incentives for each state, added to the reward signal 
                  of any agent at the corresponding state. Their goal is to maximize the total environmental reward given to the agents (excluding any rewards given by the incentives). 
                </div>
                <br/> <br/>
                  <div class="column is-full-width">
                    <img style="width: 1000px" src="./static/images/all_results.png" alt="results">
                  </div>
                    <strong>Figure 2</strong>: Performance comparison of different policymaker methods across the Harvest, Clean Up, and CER environments. Each plot displays the 
                    principal's reward over validation timesteps. Dashed lines represent principal payoff upon convergence to a policy. In addition to two frontier LLMs, we include 
                    RL-based methods of AI Economist, MetaGrad, and three bandit algorithms: UCB, Thompson sampling, and &epsilon;-greedy. Both instantiations of the LLM principal 
                    consistently achieve higher sample efficiency than baselines across all environments. For each environment, we include a closer frame of reference for LLM performance 
                    in early timesteps (top) as well as the full run below. All methods were run on 10 seeds. 
                  <br /><br />
                  We see significant efficiency gains in our method across all three environments, converging an order of magnitude faster than AI Economist on Harvest and Cleanup, and two 
                  orders of magnitude faster on CER, with similar or greater payoffs. We provide further analysis of these results in the table below. 
                  <div class="column is-full-width">
                    <img style="width: 1000px" src="./static/images/table.png" alt="table">
                  </div>
                    <strong>Table 1</strong>: Performance Comparison Across Environments. Mean and standard error principal payoff at convergence and timestep to reach convergence, 
                    across 10 seeds for each method. Convergence timestep was determined according to relative change within a rolling window. We denote runs that do not converge
                    or achieve trivial performance with DNC, and report their payoff at final timestep.
                  <br /><br />
                  More details on the experimental setup and specific hyperparameters can be found in the paper.
            </div>
            <!-- /Where Do We Go From Here?-->
            <!-- test-->
            <!-- What Are The Steps To Get There?-->
            <div class="container is-max-desktop content">
              <h2 class="title is-3">
                <div class="toggle-button" onclick="toggleSection('conclusion')"
                  style="cursor: pointer; display: flex; justify-content: space-between; align-items: center; width: 100%;">
                  <span>Conclusion</span>
                  <span class="toggle-icon">+</span>
                </div>
              </h2>
              <div id="conclusion" class="collapse-content">
                <div class="container is-max-desktop content">
                  Our experimental results are promising, demonstrating the effectiveness of our method across three environments. We show that our LLM-based approach is significantly more sample-efficient than existing methods.
                  Our method is generalizable and tractable, and with continued development, we believe it has the potential to be applied to a wide range of economic policy-making problems. There are limitations faced by our 
                  method; scalability of prompt design, memory constraints, and the need for further interpretability are all areas that require further research. However, we believe that our method represents one of the first 
                  steps down a path that will eventually lead to broad societal benefit. We thank you for your time, and encourage you to read the full paper for more details on our method and results.
                </div>
              </div>
              <!-- /What Are The Steps To Get There??-->

            </div>
          </div>
        </div>
    </section>
  </div>







  <!-- SIMPLE SECTION-->
  <div id="simple" style="display: none;">
    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                The improvement of economic policymaking presents an opportunity for broad societal benefit,
                a notion that has inspired research towards AI-driven policymaking tools. These tools have the potential to
                process data more effectively than human policymakers, so that the decisions made by humans can be better informed.
                However, we find thad that current AI-driven policymaking tools are often limited in their ability to quickly 
                make decisions in economic simulations, and also are unable to flexibly incoroporate nuanced information in their decision-making process.
                For these reasons, we propose using Large Language Models (LLMs) as more efficient policymakers. 
              </p>
            </div>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      <!-- Introduction. -->
      <div class="container is-max-desktop content">
        <div style="height: 50px;"></div>
        <h2 class="title is-3">
          <div class="toggle-button" onclick="toggleSection('background-simple')"
            style="cursor: pointer; display: flex; justify-content: space-between; align-items: center; width: 100%;">
            <span>Background Information</span>
            <span class="toggle-icon">+</span>
          </div>
        </h2>
        <div id="background-simple" class="collapse-content">
          <div> Before we dive in, it may be helpful to broadly cover some different kinds of artificial intelligence — AI is 
            a large umbrella term, and there are many different kinds of AI that are used for different purposes. We're going to try our best
            to stay out of the weeds (and trust us, this stuff gets really complicated really quickly), but if you're interested in learning more
            about how artificial intelligence works, <a href="https://www.youtube.com/@3blue1brown/featured">3Blue1Brown</a> has some <i>excellent</i>
            introductory videos. </div>
          <br />
          <div style="overflow: auto;">
            <img src="./static/images/dog_cat.png"  alt="Dog and Cat Image" style="width: 300px; float:right; margin-left: 20px;" />
            <p>
            Artificial intelligence uses computers to perform tasks that usually require human intelligence. One way this is done is through
            <strong>neural networks</strong>, which take in some input, like images or numbers, and produces some output. A popular example of this, depicted
            on the right, uses a neural network to take in an image of a cat or a dog, and classify which group it belongs to. Large Language Models (LLMs),
            use a special type of neural network called a <strong>transformer</strong> to take in text and produce more text. Of course,
            this is a simplification, but it should suffice for our purposes. You've likely heard of, or even used, some of these LLMs before — OpenAI's 
            ChatGPT models are publicly available LLMs that have made quite the spalsh in recent years. 
          </p> 
            In order for us to understand one of the advantanges that LLMs have over other methods in AI-based policymaking, we need to talk about how AI is trained. In particular,
            we are going to talk about how LLMs are trained, and how a neural network using <i>reinforcement learning</i> (RL) is trained. We're choosing to focus
            on how RL works because that is the way that <a href="https://arxiv.org/pdf/2108.02755">AI Economist</a>, a leading AI-driven policymaking tool, is trained. 
            In RL, a neural network is trained within some environment (we'll get more into this in a little), by giving it a reward when it outputs something good,
            and a punishment when it outputs something bad. Each time the neural network updated, it gets a tiny bit better at doing the task it was trained on. 
            This process is powerful, but can take a very long time. LLMs are not trained in an environment, instead using on a massive amount of precollected text data — in the case of
            ChatGPT, almost all of the internet. When an LLM is trained, it is trained to predict the next word in a sentence, given all of the words that came before it. 
            Again, this is a simplififcation, but should help in understanding this work. Speaking of the work, let's dive in!

          </div>
        </div>
        <!--/ Introduction. -->

        <!-- Example -->
        <div class="container is-max-desktop content">
          <h2 class="title is-3">
            <div class="toggle-button" onclick="toggleSection('example-simple')"
              style="cursor: pointer; display: flex; justify-content: space-between; align-items: center; width: 100%;">
              <span>Introduction</span>
              <span class="toggle-icon">+</span>
            </div>
          </h2>
          <div id="example-simple" class="collapse-content">
            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width">
                <img style="width: 1000px" src="./static/images/diagram_simplified.png" alt="diagram">
              </div>
            </div>
            <div>This diagram outlines our approach — if you don't understand parts of it now, don't worry! We will cover everything in the following sections.
            </div>
            <br />
              <div class="container is-max-desktop content">
                <div>
                  Using AI to help make economic policy decisions certainly sounds promising — AI can process much more information than you or I can, much faster. 
                  But how do we test something like this? How do we evaluate what methods are better suited for policymaking than others? Before running any tests or 
                  experiments, we can highlight some immediate advantages that LLMs have over other methods. First, their training results in models
                  that can be used in a variety of different ways; right now, you can ask ChatGPT to write a poem, solve a math problem, or summarize text. On the other hand,
                  AI Economist has to be trained from scratch for each new situation it is used in. This also means that LLMs can more flexibly take in input, such as economic forecasts,
                  expert opinions, or broad context about the situation, whereas AI Economist is limited to whatever predefined types of inputs it was trained on. Furthermore, 
                  LLMs are able to output their rationale behind their decisions in a way that is understandable to humans, whereas other methods output only their decisions. It is worth
                  noting that there is still much work to be done in the field of LLM interpretability (ensuring any given rationale is <i>really</i> why the model made a certain decision), 
                  but it is a step in the right direction. In the next sections, we will talk about the experiments that we ran to test our claim that LLMs are better suited for policymaking.
                </div>
              </div>
          </div>
          <!-- /Example -->
          <!-- Where Do We Go From Here?-->
          <div class="container is-max-desktop content">
            <h2 class="title is-3">
              <div class="toggle-button" onclick="toggleSection('experiments-simple')"
                style="cursor: pointer; display: flex; justify-content: space-between; align-items: center; width: 100%;">
                <span>Experiments</span>
                <span class="toggle-icon">+</span>
              </div>
            </h2>
            <div id="experiments-simple" class="collapse-content">
              <div class=" is-centered  container is-max-desktop content">

                  The reasons outlined above provided our motivation for using LLMs as policymakers, but we still need to test our hypothesis. In order to do this, we utilized three different economic
                  simulations and compared the performance of our method to several baseline methods. These baseline methods consist of AI Economist, MetaGrad (another AI-driven policymaking tool),
                  and three algorithms commonly used as comparisons in similar experiments (the specifics of these are outside of the scope of this simplified discussion, so we'll avoid talking about
                  them much here).
                  <br /><br />
                  Each simulation has a certain number of <strong>players</strong> populating an <strong>environment</strong>. Note that 'players' are typically referred to as 'agents' in the field of AI, 
                  but the term 'players' is more intuitive for people unfamiliar with AI. Stay with us here — each player is itself an AI, and learns to take actions in the environment that maximize
                  its reward. The players are 'selfish', and care only about maximizing their <i>own </i> reward. To best evaluate the policymakers, we chose environments in which players without 
                  external influence would act in a way that negatively affected the group as a whole. We then introduced the policymakers and gave them the goal of improving group welfare. If you're 
                  confused, fear not! Everything should become clearer as we explain the environments.
                  <br /> <br/>

                  <div style="overflow: auto;">
                    <img src="./static/images/social_dilemma.png"  alt="Harvest" style="width: 300px; float:right; margin-left: 20px;" />
                    <strong>Harvest</strong>: In this environment, apples grow in patches. Players are given a reward when they pick an apple. Apples 
                    can regrow, but only if there are still other apples in the patch. This is the source of the social dilemma in this environment; 
                    if two players are looking at the last apple in a patch, they each have an incentive to pick it and get their reward before the other player does.
                    This dilemma results in all of the apples being quickly harvested, and the players are left with no apples to pick. In this environment, the goal of
                    the policymakers is to maximize the total number of apples harvested in the simulation by creating three tax rates. These tax rates apply to 
                    a player that picks an apple based on how many apples they have recently harvested, and splits the taxed reward between all players. With a good set
                    of tax rates, the players learn to let apples regrow, thus maximizing the total number of apples harvested.
                  </div>
                  <br /><br />
                  <div style="overflow: auto;"></div>
                    <img src="./static/images/cleanup.png"  alt="Clean UP" style="width: 300px; float:right; margin-left: 20px;" />
                    <strong>Clean Up</strong>: Similarly to Harvest, this environment contains apples that give reward to players that pick them. 
                    Here, apple regrowth is not linked to surrounding apples, but rather the level of pollution in the river, which builds throughout the simulation.
                    Too much pollution in the river means that apples won't grow back at all. Players can clean up the river, but they are punished for doing so.
                    Without policymakers, the players harvest all available apples, but avoid cleaning the river. Again, the goal of the policymakers here is to
                    maximize the total number of apples harvested in the simulation. In Clean Up, policymakers output three incentives that are added to the reward of
                    players that harvest an apple, clean the river, or take some other action. A good set of incentives must carefully balance these incentives to ensure
                    that players clean enough to ensure apple regrowth without ignoring harvesting <br/> altogether. 
                  </div>
                  <br /><br />
                  <div style="overflow: auto;">
                    <img src="./static/images/CER.png"  alt="Clean UP" style="width: 300px; float:right; margin-left: 20px;" />
                    <strong>Contextual Escape Room</strong> (CER): Our final environment is a bit different from the previous two. In this environment, five players are placed in a starting position,
                    and can move to four other positions — three levers, and a door. In every simulation, one lever is randomly 'activated'. If two players pull the activated lever, the door
                    opens, and any players at the door receive a large reward. However, the players at the lever are given a small punishment. As a result, players without external influence
                    always go to the door, leaving nobody to pull the lever. In this environment, policymakers create incentives for each of the five total positions, added to the reward 
                    of any player in each position. Their goal is to maximize the total environmental reward given to the agents (this excludes any rewards given by the incentives). 
                  </div>
                  <br /><br />
                  In the next section, we'll talk about the performance of our method on these environments, and how it compares to the other baseline methods.
            </div>

            <!-- /Where Do We Go From Here?-->
            <div class="container is-max-desktop content">
              <h2 class="title is-3">
                <div class="toggle-button" onclick="toggleSection('results-simple')"
                  style="cursor: pointer; display: flex; justify-content: space-between; align-items: center; width: 100%;">
                  <span>Results</span>
                  <span class="toggle-icon">+</span>
                </div>
              </h2>
              <div id="results-simple" class="collapse-content">
                <div class="container is-max-desktop content">
                  <div class="columns is-centered has-text-centered">
                    <div class="column is-full-width">
                      <img style="width: 1000px" src="./static/images/results_simplified.png" alt="diagram">
                  </div>
                </div>
                <div> So, how'd we do? Not too shabby, it turns out. The results of our experiments are shown in the figure above. Each column of graphs represents one of our environments. 
                  On the y-axis (vertical), we have the reward of the policymaker (remember, that's total apples harvested for Harvest and Clean Up, and total environmental reward for CER). On
                  the x-axis (horizontal), we have the number of timesteps — you can roughly think of this as the how many simulations were run. The dark and light blue lines represent the two different
                  LLMs that we used, the red line represents AI Economist, and the orange line represents MetaGrad. The other lines represent the comparison algorithms that we briefly touched on earlier.
                  <br /><br />
                  Our method achieved very high efficiency in all three environments, meaning that it was able to reach a high reward in less time than the other methods. This is the reason for the zoomed in
                  graphs in the top column; we wanted to show how rapidly the LLMs were able to learn in comparison to the baselines. You can see that in CER, the LLMs were able to reach a high reward (actually,
                  close to the highest possible reward) after just one attempt. All in all, our experimental results support our hypothesis that LLMs represent a promising path to more efficient and effective policymaking.
                  So... what now? We'll talk about that in the next section. 
              </div>
            </div>
            
          </div>
          <div class="container is-max-desktop content">
            <h2 class="title is-3">
              <div class="toggle-button" onclick="toggleSection('what-now-simple')"
                style="cursor: pointer; display: flex; justify-content: space-between; align-items: center; width: 100%;">
                <span>What Now?</span>
                <span class="toggle-icon">+</span>
              </div>
            </h2>
            <div id="what-now-simple" class="collapse-content">
              <div class="is-centered container is-max-desktop content">
                Our results are promising, but there is clearly still much work to be done — little colorful players running around and collecting apples are a far cry from actual policymaking.
                The field of AI-based policymaking tools is still in its infancy, but there is a clear path forward to eventual real-world implementation. Beyond the specifics of <i>any</i>
                method, the simulations themselves need to increase in complexity and realism in order to better evaluate the potential of proposed methods. Much work is already being done to this end,
                some of which utilizes LLMs as the players themselves, able to have complex social interactions in a way that better models human behavior. Towards the development of LLM policymakers, 
                there are several avenues that merit exploration. These are fairly dense topics, but we will give a few examples. Firstly, though these LLMs have already been trained, the concept of
                'fine-tuning' models (further training towards a specific task) holds promise in improving policymaking performance. Also, a limitation of LLMs today is their context window — the amount of
                information they can consider at once. This limitation is actively being worked on and improved. LLMs have taken the world by storm, and research towards their improvement is constantly
                uncovering ways to make them more efficient and effective. Furthermore, the reliability of these models is of the utmost importance, and huge amounts of money and time are being spent to
                ensure their safety. We believe that continued work in the field of AI-based policymaking stands to greatly benefit society, improving the lives of all. If you made it this far, thank you 
                for sticking with us! We hope that you've learned something new, and that you're excited about the future of AI in policymaking.
            </div>
        </div>
    </section>
  </div>

  <div id="TLDR" style="display: block;">
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Brief Summary</h2>
            <div class="subtitle has-text-justified hero-teaser">
              <ol>
                <li> The improvement of economic policymaking has the potential to greatly benefit society. However, policymaking is hard. </li>
                <li> This had led researchers to pursue AI driven solutions, better able to process the massive amounts of data.</li>
                <li> However, these existing solutions are inefficient, taking many iterations to find optimal policies in simulation.
                </li>
                <li>We introduce the usage of Large Language Models in place of current AI-driven methods. In our experiments, we found 
                  that these models are more flexible and efficient
                  in their problem solving. </li>
              </ol>
            </div>
          </div>
        </div>
      </div>
    </section>
  </div>


  <!--/ Animation. -->
<!-- 
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{zhang2024sed,
title={Social Environment Design},
author={Edwin Zhang and Sadie Zhao and Tonghan Wang and Safwan Hossain and Henry Gasztowtt and Stephan Zheng and David C. Parkes and Milind Tambe and Yiling Chen},
booktitle={International Conference on Machine Learning},
year={2024},
url={https://arxiv.org/abs/2402.14090}
}</code></pre>
    </div>
  </section> -->
<section class="section" id="Ethics">
    <div class="container is-max-desktop content">
      <h2 class="title">Ethical Considerations</h2>

    Though we strongly believe that research in this field has the potential to benefit society broadly,
     we also realize that the development of these systems comes with risk. Though that this field is still years a
     way from the technological possibility of real-world implementation, it is essential to remember that technological 
     difficulty should not be considered the lone obstacle. Equally importantly, if not more so, these systems must be rigorously 
     evaluated for any existent biases imparted by developers or training data. Furthermore, defining social welfare in the real 
     world will be an involved task, and should likely be multifaceted, unlike our naive implementation. The continued development o
     f this field should involve a diverse set of people from different backgrounds, professions, and viewpoints.
      Progress should be fully transparent, and made digestible to those outside of technical research.
      </div>
  </section> 

  <footer class="footer">
    <div class="container">

      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p style="text-align: center
            ">
              Website templated borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies.</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>



</body>

</html>
